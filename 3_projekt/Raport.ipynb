{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeligencja obliczeniowa\n",
    "## Projekt 3: TS 1 - Podstawy Gymnasium\n",
    "Olgierd Piofczyk, Kaja Dzielnicka\n",
    "\n",
    "Zaimplementowano rozwiązywanie problemu w środowisku [`Taxi-v3` z Gymnasium](https://gymnasium.farama.org/environments/toy_text/taxi/).\n",
    "Agentem jest taksówka, mająca za zadanie przewożenie pasażerów między wyznaczonymi miejscami na siatce 5x5. Agent może wykonać jedną z sześciu możliwych akcji: ruch na północ, południe, wschód, zachód, podjęcie pasażera oraz wysadzenie pasażera.\n",
    "\n",
    "## Algorytm Q-learning\n",
    "Zastosowano uczenie ze wzmocnieniem, konkretnie algorytmu [Q-learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "W metodzie Q-learning kluczową role odgrywa tablica dyktująca agenta. Na początku inicjalizuje się ją bliskimi zera wartościami. Następnie agent w każdym kroku wybiera najlepszą wartość z tablicy Q i realizuje odpowiadający jej ruch. Do tego istnieje pewna szansa na zignorowanie tablicy i wykonanie losowego legalnego ruchu (tzw. eksploracja). Szansa na losową eksplorację maleje z kolejnymi epokami. Agent po wykonaniu ruchu dostaje ze środowiska nagrodę i na jej podstawie aktualizuje zawartość tablicy Q.\n",
    "\n",
    "Algorytm Q-learning nie wymaga modelowania problemu, opiera się jedynie na doświadczeniach agenta.\n",
    "\n",
    "## Implementacja\n",
    "Implementacja skryptu rozpoczyna się od inicjalizacji środowiska, tabeli Q oraz parametrów algorytmu (np. współczynnik uczenia równy 0.9, współczynnik dyskontowania równy 0.9, szansę eksploracji początkowo równą 0.02 i malejącą w każdym kroku). W trakcie każdego epizodu agent podejmuje akcje bazując na obecnej polityce, następnie obserwuje nagrodę i nowy stan, po czym aktualizuje tabelę Q zgodnie z równaniem\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "Po zakończeniu epizodu wartość epsilon jest zmniejszana, co z czasem skutkuje mniejszą eksploracją i większą eksploatacją nauczonej polityki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def run(episodes, is_training=True, render=False):\n",
    "    env = gym.make('Taxi-v3', render_mode='human' if render else None)\n",
    "\n",
    "    if(is_training):\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:\n",
    "        f = open('taxi.pkl', 'rb')\n",
    "        q = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    learning_rate_a = 0.9\n",
    "    discount_factor_g = 0.9\n",
    "    epsilon = 0.02\n",
    "    epsilon_decay_rate = 0.001\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while(not terminated and not truncated):\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "            else:\n",
    "                action = np.argmax(q[state,:])\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                q[state,action] += learning_rate_a * (reward + discount_factor_g * np.max(q[new_state,:]) - q[state,action])\n",
    "            state = new_state\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        rewards_per_episode[i] = reward\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "    plt.plot(sum_rewards)\n",
    "    plt.show()\n",
    "    plt.savefig('taxi.png')\n",
    "\n",
    "    if is_training:\n",
    "        f = open(\"taxi.pkl\",\"wb\")\n",
    "        pickle.dump(q, f)\n",
    "        f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run(15000)\n",
    "\n",
    "\n",
    "    # run(100, is_training=False, render=True)\n",
    "    run(300, is_training=True, render=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
