{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeligencja obliczeniowa\n",
    "\n",
    "**Projekt 5: Przestrzenie Ciągłe**   \n",
    "Olgierd Piofczyk, Kaja Dzielnicka\n",
    "\n",
    "---\n",
    "\n",
    "## Środowisko: Pendulum-v1\n",
    "\n",
    "W ramach projektu zastosowaliśmy środowisko `Pendulum-v1` z biblioteki `gym`. Jest to klasyczny problemem sterowania, w którym zadaniem agenta jest balansowanie wahadła w pionie, startując z dowolnej pozycji. Agent otrzymuje negatywne nagrody za odchylenia od pionu oraz za dużą prędkość kątową.\n",
    "\n",
    "## Algorytm: SAC\n",
    "\n",
    "SAC jest algorytmem wzmacniającego uczenia, który łączy metody aktora-krytyka z maksymalizacją entropii. Celem jest nie tylko maksymalizacja nagrody, ale także eksploracja różnych działań.\n",
    "\n",
    "## Implementacja\n",
    "\n",
    "### Instalacja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zdefiniowanie folderu do zapisywania wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = 'logs'\n",
    "learning_algorithm = 'SAC'\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "if not os.path.exists(learning_algorithm):\n",
    "    os.makedirs(learning_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoda uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, max_iters=10, timesteps=50000):\n",
    "\n",
    "    model = SAC('MlpPolicy', env, verbose=1, tensorboard_log=logs_dir)\n",
    "    name = f'{learning_algorithm}/'\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        model.learn(total_timesteps=timesteps, reset_num_timesteps=False)\n",
    "        model.save(f'{name}{timesteps*iters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoda testowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, path_to_model):\n",
    "    model = SAC.load(path_to_model)\n",
    "\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    while True:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, _, done, _, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja środowiska i trening agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=None)\n",
    "env_test = gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "train(env, 'SAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(env_test, 'SAC', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
