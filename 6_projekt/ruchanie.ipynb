{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeligencja obliczeniowa\n",
    "## Projekt 3: Problemy wieloagentowe\n",
    "Olgierd Piofczyk, Kaja Dzielnicka\n",
    "\n",
    "## 1. Wstęp\n",
    "Kółko i krzyżyk to klasyczna gra dla dwóch graczy, w której gracze na zmianę zaznaczają pola na planszy 3x3. Celem jest umieszczenie trzech swoich znaków w jednej linii: poziomej, pionowej lub ukośnej. Niniejsze sprawozdanie przedstawia implementację algorytmu uczenia przez wzmacnianie (RL) do rozwiązania środowiska Tic-Tac-Toe przy użyciu biblioteki PettingZoo.\n",
    "\n",
    "## 2. Konfiguracja Środowiska\n",
    "Biblioteka PettingZoo zapewnia gotowe środowisko Tic-Tac-Toe, które można wykorzystać do trenowania i testowania agentów. Do implementacji użyto środowiska `pettingzoo.classic.tictactoe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Użyty Algorytm\n",
    "Do rozwiązania problemu wybrano algorytm DQN (Deep Q-Network). Algorytm DQN wykorzystuje sieci neuronowe do aproksymacji funkcji wartości Q, co umożliwia agentowi podejmowanie optymalnych decyzji w danym stanie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eksperymenty i Wyniki\n",
    "Aby ocenić wydajność agenta, przeprowadzono serię eksperymentów, w których agent rozgrywał gry przeciwko sobie i prostym agentom losowym. Wyniki eksperymentów przedstawiono na krzywej uczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krzywa uczenia pokazuje, jak całkowita nagroda zdobywana przez agenta zmienia się w trakcie trwania treningu. Wzrost wartości nagród wskazuje na poprawę strategii agenta w miarę postępu treningu.\n",
    "\n",
    "## 5. Wnioski\n",
    "Implementacja algorytmu DQN w środowisku Tic-Tac-Toe z PettingZoo pozwoliła na efektywne trenowanie agenta do gry. Eksperymenty wykazały, że agent potrafi uczyć się optymalnych strategii gry, co zostało potwierdzone przez rosnące wartości nagród na krzywej uczenia. W przyszłości można by rozważyć bardziej zaawansowane algorytmy lub kombinacje różnych podejść do jeszcze lepszego wyniku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
